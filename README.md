# micrograd-clone

# Micrograd-Clone

This repository contains my own implementation of the `micrograd` project. `micrograd` is a tiny Autograd engine (and a neural network library that uses this engine) that implements backpropagation for training neural networks.

## What I've Done

In this clone of `micrograd`, I have:

- Implemented the core Autograd engine that can compute gradients for a variety of operations.
- Built a simple neural network library on top of the Autograd engine.
- Created a few example neural networks using this library and trained them on some sample datasets.
- Debugged and fixed several issues in the original `micrograd` code.
- Added some additional features and improvements to the original `micrograd` project.

## How to Use

To use this project, you can clone the repository and import the `micrograd` module in your Python scripts. See the example scripts in the `examples` directory for usage examples.

## Future Work

I plan to continue improving and expanding this project in the future. Some potential areas for future work include adding support for more operations, optimizing the performance of the Autograd engine, and improving the usability of the neural network library.